{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/henningheyen/.cache/huggingface/datasets/SetFit___json/SetFit--mnli-12154829fe6f4c49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7901edf6ce854445857a7517979c6097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SetFit/mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "# nli-deberta-v3-base for NLI\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-base', use_fast= False)\n",
    "\n",
    "features = tokenizer(['A man is eating pizza', 'A black race car starts up in front of a crowd of people.'], ['A man eats something', 'A man is driving down a lonely road.'],  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': \"What's truly striking, though, is that Jobs has never really let this idea go.\",\n",
       " 'text2': 'Jobs never held onto an idea for long.',\n",
       " 'label': 2,\n",
       " 'idx': 41,\n",
       " 'label_text': 'contradiction'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 41\n",
    "\n",
    "premise1 = dataset['validation'][i]['text1']\n",
    "hypothesis1 = dataset['validation'][i]['text2']\n",
    "label1 = dataset['validation'][i]['label']\n",
    "label_text1 = dataset['validation'][i]['label_text']\n",
    "dataset['validation'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': 'Conversely, an increase in government saving adds to the supply of resources available for investment and may put downward pressure on interest rates.',\n",
       " 'text2': 'Interest rates should increase to increase saving.',\n",
       " 'label': 2,\n",
       " 'idx': 32,\n",
       " 'label_text': 'contradiction'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 32\n",
    "\n",
    "premise2 = dataset['validation'][j]['text1']\n",
    "hypothesis2 = dataset['validation'][j]['text2']\n",
    "label2 = dataset['validation'][j]['label']\n",
    "label_text2 = dataset['validation'][j]['label_text']\n",
    "dataset['validation'][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  tensor([[ 4.7858, -2.6995, -1.9693],\n",
      "        [ 4.6272, -3.1961, -0.7425]])\n",
      "scores.argmax(dim=1):  tensor([0, 0])\n",
      "labels: ['contradiction', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "# Example on MNLI\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-base', use_fast= False)\n",
    "\n",
    "features = tokenizer([(premise1, hypothesis1), (premise2, hypothesis2)], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print('scores: ', scores)\n",
    "    label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "    print('scores.argmax(dim=1): ', scores.argmax(dim=1))\n",
    "    print('labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on more MNLI samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_set(size):\n",
    "    \n",
    "    # replacing labels according to last layer in the models\n",
    "    new_labels = [2 if label == 1 else 1 if label == 0 else 0 for label in dataset['validation']['label']]\n",
    "\n",
    "    test_set = [(dataset['validation'][i]['text1'], dataset['validation'][i]['text2']) for i in range(size)]\n",
    "    test_labels = new_labels[:size]\n",
    "    test_labels_text = dataset['validation'][:size]['label_text']\n",
    "\n",
    "    return {'sentence_pairs': test_set, 'test_labels': test_labels, 'test_labels_text': test_labels_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_test_set() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[43mmake_test_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m41\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: make_test_set() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "test_set = make_test_set(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The new rights are nice enough', 'Everyone really likes the newest benefits '), ('This site includes a list of all award winners and a searchable database of Government Executive articles.', 'The Government Executive articles housed on the website are not able to be searched.'), (\"uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\", 'I like him for the most part, but would still enjoy seeing someone beat him.'), (\"yeah i i think my favorite restaurant is always been the one closest  you know the closest as long as it's it meets the minimum criteria you know of good food\", 'My favorite restaurants are always at least a hundred miles away from my house. '), (\"i don't know um do you do a lot of camping\", 'I know exactly.'), (\"well that would be a help i wish they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be\", 'We have plenty of space in the landfill.'), ('yeah i know and i did that all through college and it worked too', 'I did that all through college but it never worked '), (\"Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\", \"Most of Mrinal Sen's work can be found in European collections.\"), ('If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock in her profit on the shares at $18, less whatever the options cost.', 'THe strike price could be $8.'), ('3)  Dare you rise to the occasion, like Raskolnikov, and reject the petty rules that govern lesser men?', 'Would you rise up and defeaat all evil lords in the town?'), ('The most important directions are simply up and up leads eventually to the cathedral and fortress commanding the hilltop, and down inevitably leads to one of three gates through the wall to the new town.', 'Go downwards to one of the gates, all of which will lead you into the cathedral.'), ('The bhakti movement of the Tamils brought a new warmth to the hitherto rigid Brahmanic ritual of Hinduism.', \"The Tamils' bhakti movement froze the previously warm ritual of Hinduism.\"), (\"see now in a situation like that the boys are only sixteen years old and they were sexually involved with her and i think like at that particular point she was twenty three you know so she wasn't really that much older than them and being a boy at that age i think that they're very um you know let's face it that's at a point in your life when you you're just starting to realize all the things of life\", 'Everyone involved was the same age.'), ('You and your friends are not welcome here, said Severn.', 'Severn said the people were not welcome there.'), (\"um-hum um-hum yeah well uh i can see you know it's it's it's it's kind of funny because we it seems like we loan money you know we money with strings attached and if the government changes and the country that we loan the money to um i can see why the might have a different attitude towards paying it back it's a lot us that  you know we don't really loan money to to countries we loan money to governments and it's the\", \"We don't loan a lot of money.\"), (\"i'm not sure what the overnight low was\", \"I don't know how cold it got last night.\"), ('so i have to find a way to supplement that', 'I need a way to add something extra.'), (\"the hologram makes up all these things and uh i mean sometimes  sometimes it's funny sometimes it's not but uh you know it's something to pass the time until we do and then and then we watch football\", 'Sometimes it is amusing to see what the hologram creates.'), ('5 The share of gross national saving used to replace depreciated capital has increased over the past 40 years.', 'Gross national saving was highest this year.'), ('So far, however, the number of mail pieces lost to alternative bill-paying methods is too small to have any material impact on First-Class volume.', 'The amount of lost mail is huge and really impacts mail volume')]\n",
      "[2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 1, 1, 2, 0]\n",
      "['neutral', 'contradiction', 'entailment', 'contradiction', 'contradiction', 'contradiction', 'contradiction', 'neutral', 'contradiction', 'neutral', 'contradiction', 'contradiction', 'contradiction', 'entailment', 'neutral', 'entailment', 'entailment', 'entailment', 'neutral', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "print(test_set['sentence_pairs'][:20])\n",
    "print(test_set['test_labels'][:20])\n",
    "print(test_set['test_labels_text'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_mnli(model_name, test_set):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'cross-encoder/{model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'cross-encoder/{model_name}', use_fast= False)\n",
    "\n",
    "    features = tokenizer(test_set, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(**features).logits\n",
    "        label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "        labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "        return scores.argmax(dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 31s, sys: 1min 59s, total: 6min 30s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deberta_xsmall_pred = get_predictions_mnli('nli-deberta-v3-xsmall', test_set['sentence_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsmall: 0.872\n"
     ]
    }
   ],
   "source": [
    "print('xsmall:', accuracy_score(test_set['test_labels'], deberta_xsmall_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 2min 29s, total: 9min 39s\n",
      "Wall time: 11min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deberta_small_pred = get_predictions_mnli('nli-deberta-v3-small', test_set['sentence_pairs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small: 0.879\n"
     ]
    }
   ],
   "source": [
    "print('small:', accuracy_score(test_set['test_labels'], deberta_small_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 40s, sys: 5min 12s, total: 19min 52s\n",
      "Wall time: 1d 21h 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deberta_base_pred = get_predictions_mnli('nli-deberta-v3-base', test_set['sentence_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: 0.896\n"
     ]
    }
   ],
   "source": [
    "print('base:', accuracy_score(test_set['test_labels'], deberta_base_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 19s, sys: 5min 7s, total: 28min 27s\n",
      "Wall time: 1h 16min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deberta_large_pred = get_predictions_mnli('nli-deberta-v3-large', test_set['sentence_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large: 0.898\n"
     ]
    }
   ],
   "source": [
    "print('large:', accuracy_score(test_set['test_labels'], deberta_large_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsmall: 0.954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('xsmall:', accuracy_score(test_set['test_labels'], deberta_xsmall_pred))\n",
    "#print('small:', accuracy_score(test_set['test_labels'], deberta_small_pred))\n",
    "#print('base:', accuracy_score(test_set['test_labels'], deberta_base_pred))\n",
    "#print('large:', accuracy_score(test_set['test_labels'], deberta_large_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 1000 validation examples. Accuracy scores:\n",
    "- 0.872 xsmall\n",
    "- 0.879 small\n",
    "- 0.896 base\n",
    "- 0.898 large (500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xsmall: 0.954 for 1000 train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['deberta_xsmall', 'deberta_small', 'deberta_base', 'deberta_large']\n",
    "predictions = [deberta_xsmall_pred, deberta_small_pred, deberta_base_pred, deberta_large_pred]\n",
    "scores = [evaluate(test_labels, predicted_labels) for predicted_labels in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(scores)\n",
    "df['model'] = model_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ZeroShotNLI\n",
    "from utils import make_test_set\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/henningheyen/.cache/huggingface/datasets/SetFit___json/SetFit--mnli-12154829fe6f4c49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab3c25e7f944a99b077f6ecdedba76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set = make_test_set(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsmall = ZeroShotNLI('nli-deberta-v3-small')\n",
    "\n",
    "probs_xsmall = xsmall.predict_for_lime(sentence_pairs = test_set['sentence_pairs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['contradiction', 'entailment', 'neutral']\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "def show_lime(sentence_pairs, predict, num_samples=100, num_features=10):\n",
    "  \n",
    "  for sentence_pair in sentence_pairs:\n",
    "    text_instance = sentence_pair[0] + \" [SEP] \" + sentence_pair[0]\n",
    "    explanation = compute_explanation(text_instance, predict, num_samples=num_samples, num_features=num_features)\n",
    "    print(\"sentence_pair: \", text_instance)\n",
    "    print(\"LIME Explanation:\")\n",
    "    explanation.show_in_notebook(text=True)\n",
    "    print('-----------------------')\n",
    "\n",
    "def compute_explanation(sentence_pair, predict, num_samples=100, num_features=10):\n",
    "  return explainer.explain_instance(sentence_pair, predict, num_samples=num_samples, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_pair:  Impossible. [SEP] Impossible.\n",
      "LIME Explanation:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_lime(test_set[\u001b[39m'\u001b[39;49m\u001b[39msentence_pairs\u001b[39;49m\u001b[39m'\u001b[39;49m][:\u001b[39m2\u001b[39;49m], xsmall\u001b[39m.\u001b[39;49mpredict_for_lime, num_samples\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mshow_lime\u001b[0;34m(sentence_pairs, predict, num_samples, num_features)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msentence_pair: \u001b[39m\u001b[39m\"\u001b[39m, text_instance)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLIME Explanation:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m explanation\u001b[39m.\u001b[39;49mshow_in_notebook(text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-----------------------\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/masterproject/lib/python3.11/site-packages/lime/explanation.py:195\u001b[0m, in \u001b[0;36mExplanation.show_in_notebook\u001b[0;34m(self, labels, predict_proba, show_predicted_value, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Shows html explanation in ipython notebook.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[39mSee as_html() for parameters.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39mThis will throw an error if you don't have IPython installed\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display, HTML\n\u001b[0;32m--> 195\u001b[0m display(HTML(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mas_html(labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    196\u001b[0m                           predict_proba\u001b[39m=\u001b[39;49mpredict_proba,\n\u001b[1;32m    197\u001b[0m                           show_predicted_value\u001b[39m=\u001b[39;49mshow_predicted_value,\n\u001b[1;32m    198\u001b[0m                           \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/masterproject/lib/python3.11/site-packages/lime/explanation.py:269\u001b[0m, in \u001b[0;36mExplanation.as_html\u001b[0;34m(self, labels, predict_proba, show_predicted_value, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m predict_proba_js \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m predict_proba:\n\u001b[1;32m    263\u001b[0m     predict_proba_js \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[39m    var pp_div = top_div.append(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[39m                        .classed(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlime predict_proba\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, true);\u001b[39m\n\u001b[1;32m    266\u001b[0m \u001b[39m    var pp_svg = pp_div.append(\u001b[39m\u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m).style(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m100\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m);\u001b[39m\n\u001b[1;32m    267\u001b[0m \u001b[39m    var pp = new lime.PredictProba(pp_svg, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m);\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39m \u001b[39m%\u001b[39m (jsonize([\u001b[39mstr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_names]),\n\u001b[0;32m--> 269\u001b[0m            jsonize(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39m))))\n\u001b[1;32m    271\u001b[0m predict_value_js \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m show_predicted_value:\n\u001b[1;32m    273\u001b[0m     \u001b[39m# reference self.predicted_value\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39m# (svg, predicted_value, min_value, max_value)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "show_lime(test_set['sentence_pairs'][:2], xsmall.predict_for_lime, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
