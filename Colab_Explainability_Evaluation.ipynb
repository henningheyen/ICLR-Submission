{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqpGnEcTNdc_"
      },
      "source": [
        "# Global Explainability Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbve2QjJZb1S"
      },
      "source": [
        "Structure:\n",
        "- Computing explanations for all models and datasets\n",
        "- evaluating global faithfulness (comprehensiveness)\n",
        "- evaluating global plausibility (IOU scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qkjvwPLbRir"
      },
      "source": [
        "## Cloning TransformerExplainability Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wyBQOl5M6NQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_N6ZN5YtXK"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/henningheyen/ICLR-Submission.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zFaE_ISbX-2",
        "outputId": "8d44255b-fa9d-4c1e-a057-7491947d1326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ICLR-Submission\n"
          ]
        }
      ],
      "source": [
        "%cd ICLR-Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olC_wq4tbhAQ"
      },
      "outputs": [],
      "source": [
        "# Installing Dependencies\n",
        "!pip install lime\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og8GVh9NH7AB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1eAxFjVPbCsA"
      },
      "outputs": [],
      "source": [
        "from utils import make_test_set_esnli, make_test_set_mnli, make_test_set_cose\n",
        "from model import ModelNLI, ModelZSC\n",
        "from explainer import Explainer\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bzawBjX3Qh-3"
      },
      "outputs": [],
      "source": [
        "# number of explanations\n",
        "NUM_EXPL = 100\n",
        "\n",
        "#importing dataset\n",
        "dataset_esnli = make_test_set_esnli(size=NUM_EXPL, remove_bad_explanations=True) # 9830 is dev size esnli (originally 9842 but 12 instance are wrongly annotated)\n",
        "dataset_mnli = make_test_set_mnli(size=NUM_EXPL) # 9815 is dev size mnli\n",
        "dataset_cose = make_test_set_cose(size=NUM_EXPL, remove_bad_explanations=True) # 718 (originally 1221 is dev size cose but 503 instances have bad explanations, i.e. the whole question highlighted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ld9tsHgBIQYv",
        "outputId": "d614e35a-ab05-4191-99d7-deb9c0b5337b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average explanation input ratio e-SNLI:  0.19858092123245383\n",
            "average explanation input ratio CoS-e:  0.2605110957380536\n"
          ]
        }
      ],
      "source": [
        "#calculating average explanation length per dataset\n",
        "\n",
        "full_dataset_esnli = make_test_set_esnli(size=9830, remove_bad_explanations=True) # 9830 is dev size esnli (originally 9842 but 12 instance are wrongly annotated)\n",
        "full_dataset_cose = make_test_set_cose(size=718, remove_bad_explanations=True) # 718 (originally 1221 is dev size cose but 503 instances have bad explanations, i.e. the whole question highlighted))\n",
        "\n",
        "len_esnli = [len(full_dataset_esnli['extractive_explanation'][i])/(len(full_dataset_esnli['sentence_pairs'][i][0].split())+len(full_dataset_esnli['sentence_pairs'][i][1].split())) for i in range(9830)]\n",
        "len_cose = [len(full_dataset_cose['extractive_explanation'][i])/len(full_dataset_cose['question'][i].split()) for i in range(718)]\n",
        "\n",
        "avg_len_esnli = np.mean(len_esnli)\n",
        "avg_len_cose = np.mean(len_cose)\n",
        "\n",
        "print('average explanation input ratio e-SNLI: ', avg_len_esnli)\n",
        "print('average explanation input ratio CoS-e: ', avg_len_cose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljp2OTxIb4_"
      },
      "source": [
        "## Calculating Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61p0HHvNIfkb"
      },
      "source": [
        "### Natural Language Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C3XyHhhWIdhX"
      },
      "outputs": [],
      "source": [
        "# Natural Language Inference\n",
        "xsmall_nli = ModelNLI(model_name='cross-encoder/nli-deberta-v3-xsmall')\n",
        "small_nli = ModelNLI(model_name='cross-encoder/nli-deberta-v3-small')\n",
        "base_nli = ModelNLI(model_name='cross-encoder/nli-deberta-v3-base')\n",
        "large_nli = ModelNLI(model_name='cross-encoder/nli-deberta-v3-large')\n",
        "\n",
        "models_nli = [\n",
        "    xsmall_nli,\n",
        "    small_nli,\n",
        "    base_nli,\n",
        "    large_nli\n",
        "]\n",
        "\n",
        "model_names_nli = [\n",
        "    'xsmall_nli',\n",
        "    'small_nli',\n",
        "    'base_nli',\n",
        "    'large_nli'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv8R6XQfI4b3"
      },
      "source": [
        "#### MNLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yqgxi6TQ09xb"
      },
      "outputs": [],
      "source": [
        "explainer_mnli = Explainer(class_names=['contradiction', 'entailment', 'neutral'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ak3_V60Ir7B",
        "outputId": "86b24956-c771-470f-f454-f69c4ad885ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 28s, sys: 1min 38s, total: 8min 7s\n",
            "Wall time: 2min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on xsmall\n",
        "explanations_xsmall_mnli = explainer_mnli.compute_explanations(\n",
        "    sentences = dataset_mnli['sentence_pairs'],\n",
        "    model=xsmall_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M9nj_vpxI-F"
      },
      "source": [
        "2min 3s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdCCr7KzIr5U",
        "outputId": "aad78208-9901-4cc3-d6b1-521391e0c997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 40s, sys: 1min 57s, total: 10min 37s\n",
            "Wall time: 2min 40s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on small\n",
        "explanations_small_mnli = explainer_mnli.compute_explanations(\n",
        "    sentences = dataset_mnli['sentence_pairs'],\n",
        "    model=small_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0hhPpLixHiw"
      },
      "source": [
        "2min 40s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB2zWiJoIr3b",
        "outputId": "3880a818-4a2d-47b8-9313-25153145b04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 17min 20s, sys: 3min 54s, total: 21min 15s\n",
            "Wall time: 5min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on base\n",
        "explanations_base_mnli = explainer_mnli.compute_explanations(\n",
        "    sentences = dataset_mnli['sentence_pairs'],\n",
        "    model=base_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt7oqkHuxGL9"
      },
      "source": [
        "5min 20s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzsTnK8LIr1B",
        "outputId": "50c109a8-21c3-4c40-cacc-a6faf6f27bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 54min 8s, sys: 8min 12s, total: 1h 2min 20s\n",
            "Wall time: 15min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on large\n",
        "explanations_large_mnli = explainer_mnli.compute_explanations(\n",
        "    sentences = dataset_mnli['sentence_pairs'],\n",
        "    model=large_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTnOdBE6xEgw"
      },
      "source": [
        "15min 38s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_WwvwLMpIrsg"
      },
      "outputs": [],
      "source": [
        "explanations_mnli = [\n",
        "    explanations_xsmall_mnli,\n",
        "    explanations_small_mnli,\n",
        "    explanations_base_mnli,\n",
        "    explanations_large_mnli,\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SkaRWgsI78g"
      },
      "source": [
        "#### e-SNLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrvji1pv0pLw"
      },
      "outputs": [],
      "source": [
        "explainer_esnli = Explainer(class_names=['contradiction', 'entailment', 'neutral'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibe1NgpIrSW",
        "outputId": "c67665b0-2755-4a6e-ef30-ecde28c65daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 38s, sys: 490 ms, total: 5min 39s\n",
            "Wall time: 1min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on xsmall\n",
        "explanations_xsmall_esnli = explainer_esnli.compute_explanations(\n",
        "    sentences = dataset_esnli['sentence_pairs'],\n",
        "    model=xsmall_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-thnkv6xCgX"
      },
      "source": [
        "1min 8s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJGLToBHIrO_",
        "outputId": "aa7ecd53-2eed-44e4-cfee-5210f2976380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 20s, sys: 1.5 s, total: 8min 21s\n",
            "Wall time: 2min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on small\n",
        "explanations_small_esnli = explainer_esnli.compute_explanations(\n",
        "    sentences = dataset_esnli['sentence_pairs'],\n",
        "    model=small_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lec1uu3xxA3M"
      },
      "source": [
        "1min 40s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4IzJNHPIrNs",
        "outputId": "87a144ee-8157-451a-b5be-e149d1bc130a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 17min 2s, sys: 6.13 s, total: 17min 8s\n",
            "Wall time: 4min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on base\n",
        "explanations_base_esnli = explainer_esnli.compute_explanations(\n",
        "    sentences = dataset_esnli['sentence_pairs'],\n",
        "    model=base_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAze074Uw_Zt"
      },
      "source": [
        "3min 35s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae-4GTZnIrLr",
        "outputId": "63a1a915-e99b-4dc5-b556-0445312a89ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 56min 14s, sys: 8min 50s, total: 1h 5min 5s\n",
            "Wall time: 16min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on large\n",
        "explanations_large_esnli = explainer_esnli.compute_explanations(\n",
        "    sentences = dataset_esnli['sentence_pairs'],\n",
        "    model=large_nli,\n",
        "    num_samples=100,\n",
        "    task='NLI',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB21XHT0w6XP"
      },
      "source": [
        "12min 45s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CWHzjDYIrHI"
      },
      "outputs": [],
      "source": [
        "explanations_esnli = [\n",
        "    explanations_xsmall_esnli,\n",
        "    explanations_small_esnli,\n",
        "    explanations_base_esnli,\n",
        "    explanations_large_esnli,\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpt_iJTDJojV"
      },
      "source": [
        "### Zero Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwj3oiT8IqmN"
      },
      "outputs": [],
      "source": [
        "# Zero Shot Classification\n",
        "xsmall_zsc = ModelZSC(model_name='cross-encoder/nli-deberta-v3-xsmall')\n",
        "small_zsc = ModelZSC(model_name='cross-encoder/nli-deberta-v3-small')\n",
        "base_zsc = ModelZSC(model_name='cross-encoder/nli-deberta-v3-base')\n",
        "large_zsc = ModelZSC(model_name='cross-encoder/nli-deberta-v3-large')\n",
        "\n",
        "models_zsc = [\n",
        "    xsmall_zsc,\n",
        "    small_zsc,\n",
        "    base_zsc,\n",
        "    large_zsc,\n",
        "]\n",
        "\n",
        "model_names_zsc = [\n",
        "    'xsmall_zsc',\n",
        "    'small_zsc',\n",
        "    'base_zsc',\n",
        "    'large_zsc',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4kFzS47SfHB"
      },
      "source": [
        "#### CoS-e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNQBGbl-Ju_A"
      },
      "outputs": [],
      "source": [
        "# no class names becuase of zero shot classification setting\n",
        "explainer_zsc = Explainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVjj7RiUJw7N",
        "outputId": "1805edb8-772e-42f7-eaec-8df24d610dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 29min 35s, sys: 17.8 s, total: 2h 29min 53s\n",
            "Wall time: 37min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on xsmall\n",
        "explanations_xsmall_cose = explainer_zsc.compute_explanations(\n",
        "   sentences = dataset_cose['question'],\n",
        "   model=xsmall_zsc,\n",
        "   num_samples=100,\n",
        "   class_names_list=dataset_cose['candidate_labels_list']\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ_1zvB5w2na"
      },
      "source": [
        "34min 35s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ_BW4hmJ9mV",
        "outputId": "2938780e-0e06-495a-a57d-f5895998c9cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3h 16min 5s, sys: 14.7 s, total: 3h 16min 19s\n",
            "Wall time: 49min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on small\n",
        "explanations_small_cose = explainer_zsc.compute_explanations(\n",
        "   sentences = dataset_cose['question'],\n",
        "   model=small_zsc,\n",
        "   num_samples=100,\n",
        "   class_names_list=dataset_cose['candidate_labels_list']\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nF2L4pHw17t"
      },
      "source": [
        "44min 28s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FZc0k5YJ-aU",
        "outputId": "e44df42f-e386-4831-f169-cddccc2a63dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6h 18min 23s, sys: 28.4 s, total: 6h 18min 51s\n",
            "Wall time: 1h 34min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on base\n",
        "explanations_base_cose = explainer_zsc.compute_explanations(\n",
        "   sentences = dataset_cose['question'],\n",
        "   model=base_zsc,\n",
        "   num_samples=100,\n",
        "   class_names_list=dataset_cose['candidate_labels_list']\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brIqCJZew0Db"
      },
      "source": [
        "1h 27min 7s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LMeRE4BIJ_VT",
        "outputId": "d2470d75-3e98-4d9c-82fc-8fa08890e2e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 18h 34min 45s, sys: 4min 1s, total: 18h 38min 46s\n",
            "Wall time: 4h 39min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Computing LIME explanations on large\n",
        "explanations_large_cose = explainer_zsc.compute_explanations(\n",
        "   sentences = dataset_cose['question'],\n",
        "   model=large_zsc,\n",
        "   num_samples=100,\n",
        "   class_names_list=dataset_cose['candidate_labels_list']\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkIK_s2wxNH"
      },
      "source": [
        "4h 35min 50s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uc-hmUIgKHUm"
      },
      "outputs": [],
      "source": [
        "explanations_cose = [\n",
        "    explanations_xsmall_cose,\n",
        "    explanations_small_cose,\n",
        "    explanations_base_cose,\n",
        "    explanations_large_cose,\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhX5HYnzKK3C"
      },
      "source": [
        "## Faithfulness (Comprehensiveness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-N-5pOSmAv"
      },
      "source": [
        "### Natural Language Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaH3_jtxKQ_B"
      },
      "source": [
        "#### MNLI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating aggregated comprehensiveness on 100 explanations\n",
        "comp_list_mnli = []\n",
        "\n",
        "for i, model in enumerate(models_nli):\n",
        "\n",
        "    print('model: ', model_names_nli[i])\n",
        "\n",
        "    comp_agg = [explainer_mnli.aggregated_metric(metric='comprehensiveness', explanation=explanations_mnli[i][j], sentence=dataset_mnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
        "\n",
        "    comp_list_mnli.append(comp_agg)\n"
      ],
      "metadata": {
        "id": "IK4u_0M0-Lla"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb5z_v_Opopp"
      },
      "outputs": [],
      "source": [
        "#new\n",
        "for i, model in enumerate(models_nli):\n",
        "    print(f'MNLI average aggregated comprehensiveness {model_names_nli[i]}: ', np.mean(comp_list_mnli[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY-Bz5x6OY1K"
      },
      "source": [
        "- MNLI average aggregated comprehensiveness xsmall_nli:  0.78477615\n",
        "- MNLI average aggregated comprehensiveness small_nli:  0.81709516\n",
        "- MNLI average aggregated comprehensiveness base_nli:  0.79582304\n",
        "- MNLI average aggregated comprehensiveness large_nli:  0.82296914\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBtO_DYQKaR0"
      },
      "source": [
        "#### e-SNLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ3jAapKKb6z"
      },
      "outputs": [],
      "source": [
        "# Calculating aggregated comprehensiveness on 100 explanations\n",
        "comp_list_esnli = []\n",
        "\n",
        "for i, model in enumerate(models_nli):\n",
        "\n",
        "    print('model: ', model_names_nli[i])\n",
        "\n",
        "    comp_agg = [explainer_esnli.aggregated_metric(metric='comprehensiveness', explanation=explanations_esnli[i][j], sentence=dataset_esnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
        "\n",
        "    comp_list_esnli.append(comp_agg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzjbeMVUKd2s"
      },
      "outputs": [],
      "source": [
        "# before\n",
        "for i, model in enumerate(models_nli):\n",
        "    print(f'e-SNLI average aggregated comprehensiveness {model_names_nli[i]}: ', np.mean(comp_list_esnli[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ5E4_9EOmI-"
      },
      "source": [
        "- e-SNLI average aggregated comprehensiveness xsmall_nli:  0.7259713\n",
        "- e-SNLI average aggregated comprehensiveness small_nli:  0.72445035\n",
        "- e-SNLI average aggregated comprehensiveness base_nli:  0.76369643\n",
        "- e-SNLI average aggregated comprehensiveness large_nli:  0.77805173\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkGoKiT-Tde8"
      },
      "source": [
        "### Zero Shot Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce-cA9icKjxd"
      },
      "source": [
        "### CoS-e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JjQWisjKlLE"
      },
      "outputs": [],
      "source": [
        "# Calculating aggregated comprehensiveness on 100 explanations\n",
        "comp_list_cose = []\n",
        "\n",
        "for i, model in enumerate(models_zsc):\n",
        "\n",
        "    print('model: ', model_names_zsc[i])\n",
        "\n",
        "    comp_agg = [explainer_zsc.aggregated_metric(metric='comprehensiveness', explanation=explanations_cose[i][j], sentence=dataset_cose['question'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='ZSC', candidate_labels=dataset_cose['candidate_labels_list'][j]) for j in range(NUM_EXPL)]\n",
        "\n",
        "    comp_list_cose.append(comp_agg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QalTgpyzACA"
      },
      "outputs": [],
      "source": [
        "for i, model in enumerate(models_zsc):\n",
        "    print(f'CoS-e average aggregated comprehensiveness {model_names_zsc[i]}: ', np.mean(comp_list_cose[i]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwWutPzi1K4R"
      },
      "source": [
        "- CoS-e average aggregated comprehensiveness xsmall_zsc:  0.30408690341748296\n",
        "- CoS-e average aggregated comprehensiveness small_zsc:  0.31567202631694574\n",
        "- CoS-e average aggregated comprehensiveness base_zsc:  0.35605754521364963\n",
        "- CoS-e average aggregated comprehensiveness large_zsc:  0.3910369690948089\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZoDbgJ-KrKQ"
      },
      "source": [
        "## Plausibility (IOU Scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBG62TrCKt5v"
      },
      "source": [
        "### e-SNLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WU1wf5F7qVS"
      },
      "outputs": [],
      "source": [
        "explanation_tokens_lists = [explainer_esnli.get_explanation_list(explanations_esnli[i], top_percent=avg_len_esnli) for i in range(len(models_nli))]\n",
        "ground_truth_list = dataset_esnli['extractive_explanation']\n",
        "\n",
        "iou_scores_esnli = []\n",
        "\n",
        "for i, model in enumerate(models_nli):\n",
        "  iou_scores_esnli.append([explainer_esnli.compute_instance_iou(explanation_tokens_lists[i][j], ground_truth_list[j]) for j in range(NUM_EXPL)])\n",
        "\n",
        "for i, model in enumerate(models_nli):\n",
        "    print(f'{model_names_nli[i]} macro_iou for {NUM_EXPL} number of explanations (CoS-e): ', np.mean(iou_scores_esnli[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtM3TOt-O2l1"
      },
      "source": [
        "- xsmall_nli macro_iou for 100 number of explanations (e-SNLI):  0.27125388337153045\n",
        "- small_nli macro_iou for 100 number of explanations (e-SNLI):  0.25177114388879096\n",
        "- base_nli macro_iou for 100 number of explanations (e-SNLI):  0.24971816908581615\n",
        "- large_nli macro_iou for 100 number of explanations (e-SNLI):  0.24985357779475428\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAnRCT8iKwU9"
      },
      "source": [
        "### CoS-e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWNw591L47No"
      },
      "outputs": [],
      "source": [
        "explanation_tokens_lists = [explainer_zsc.get_explanation_list(explanations_cose[i], top_percent=avg_len_cose) for i in range(len(models_zsc))]\n",
        "ground_truth_list = dataset_cose['extractive_explanation']\n",
        "\n",
        "iou_scores_cose = []\n",
        "\n",
        "for i, model in enumerate(models_zsc):\n",
        "  iou_scores_cose.append([explainer_zsc.compute_instance_iou(explanation_tokens_lists[i][j], ground_truth_list[j]) for j in range(NUM_EXPL)])\n",
        "\n",
        "for i, model in enumerate(models_zsc):\n",
        "    print(f'{model_names_zsc[i]} macro_iou for {NUM_EXPL} number of explanations (CoS-e): ', np.mean(iou_scores_cose[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGXR3-P8PCQ8"
      },
      "source": [
        "- xsmall_zsc macro_iou for 100 number of explanations (CoS-e):  0.23265864492404734\n",
        "- small_zsc macro_iou for 100 number of explanations (CoS-e):  0.23059835798876044\n",
        "- base_zsc macro_iou for 100 number of explanations (CoS-e):  0.23547574449114694\n",
        "- large_zsc macro_iou for 100 number of explanations (CoS-e):  0.23037491476531724\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPsGPW2Rh03p"
      },
      "source": [
        "# Serialise objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsS5MnXlBY7O"
      },
      "outputs": [],
      "source": [
        "explainability_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIS8zaoLBa5k"
      },
      "outputs": [],
      "source": [
        "explainability_results['mnli'] = {\n",
        "        #'dataset': dataset_mnli,\n",
        "        'xsmall': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_xsmall_mnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_mnli[0],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_mnli[0]),\n",
        "            },\n",
        "        },\n",
        "        'small': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_small_mnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_mnli[1],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_mnli[1]),\n",
        "            },\n",
        "        },\n",
        "        'base': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_base_mnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_mnli[2],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_mnli[2]),\n",
        "            },\n",
        "        },\n",
        "        'large': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_large_mnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_mnli[3],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_mnli[3]),\n",
        "            },\n",
        "        },\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3MNfBIuBrWq"
      },
      "outputs": [],
      "source": [
        "explainability_results['esnli'] = {\n",
        "        #'dataset': dataset_esnli,\n",
        "        'xsmall': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_xsmall_esnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_esnli[0],\n",
        "                'macro_comprehensiveness': float(np.mean(comp_list_esnli[0])),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_esnli[0],\n",
        "                'macro_iou': float(np.mean(iou_scores_esnli[0])),\n",
        "            },\n",
        "        },\n",
        "        'small': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_small_esnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_esnli[1],\n",
        "                'macro_comprehensiveness': float(np.mean(comp_list_esnli[1])),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_esnli[1],\n",
        "                'macro_iou': float(np.mean(iou_scores_esnli[1])),\n",
        "            },\n",
        "        },\n",
        "        'base': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_base_esnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_esnli[2],\n",
        "                'macro_comprehensiveness': float(np.mean(comp_list_esnli[2])),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_esnli[2],\n",
        "                'macro_iou': float(np.mean(iou_scores_esnli[2])),\n",
        "            },\n",
        "        },\n",
        "        'large': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_large_esnli],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_esnli[3],\n",
        "                'macro_comprehensiveness': float(np.mean(comp_list_esnli[3])),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_esnli[3],\n",
        "                'macro_iou': float(np.mean(iou_scores_esnli[3])),\n",
        "            },\n",
        "        },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqGEHMkJB3FM"
      },
      "outputs": [],
      "source": [
        "explainability_results['cose'] = {\n",
        "        #'dataset': dataset_cose,\n",
        "        'xsmall': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_xsmall_cose],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_cose[0],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_cose[0]),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_cose[0],\n",
        "                'macro_iou': np.mean(iou_scores_cose[0]),\n",
        "            },\n",
        "        },\n",
        "        'small': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_small_cose],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_cose[1],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_cose[1]),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_cose[1],\n",
        "                'macro_iou': np.mean(iou_scores_cose[1]),\n",
        "            },\n",
        "        },\n",
        "        'base': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_base_cose],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_cose[2],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_cose[2]),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_cose[2],\n",
        "                'macro_iou': np.mean(iou_scores_cose[2]),\n",
        "            },\n",
        "        },\n",
        "#        'large': {\n",
        "#            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_large_cose],\n",
        "#            'explanations_as_html':[explanation.as_html(label= explanation.top_labels[0]) for explanation in explanations_large_cose],\n",
        "#            'faithfulness': {\n",
        "#                'comprehensiveness': comp_list_cose[3],\n",
        "#                'macro_comprehensiveness': np.mean(comp_list_cose[3]),\n",
        "#            },\n",
        "#            'plausibility': {\n",
        "#                'iou': iou_scores_cose[3],\n",
        "#                'macro_iou': np.mean(iou_scores_cose[3]),\n",
        "#            },\n",
        "#        },\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAjTvWSQyD4I"
      },
      "outputs": [],
      "source": [
        "explainability_results = {}\n",
        "explainability_results['cose'] = {\n",
        "        #'dataset': dataset_cose,\n",
        "        'large': {\n",
        "            'explanations_as_list':[explanation.as_list(label= explanation.top_labels[0]) for explanation in explanations_large_cose],\n",
        "            'faithfulness': {\n",
        "                'comprehensiveness': comp_list_cose[0],\n",
        "                'macro_comprehensiveness': np.mean(comp_list_cose[0]),\n",
        "            },\n",
        "            'plausibility': {\n",
        "                'iou': iou_scores_cose[0],\n",
        "                'macro_iou': np.mean(iou_scores_cose[0]),\n",
        "            },\n",
        "        }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePAlKiHORyg"
      },
      "outputs": [],
      "source": [
        "# converting np.float32 types to float for serializing\n",
        "def check_and_convert_types(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            obj[key] = check_and_convert_types(value)\n",
        "    elif isinstance(obj, list):\n",
        "        for i in range(len(obj)):\n",
        "            obj[i] = check_and_convert_types(obj[i])\n",
        "    elif isinstance(obj, np.float32):  # Replace np.float32 with whatever type you want to check for\n",
        "        return float(obj)  # Convert to Python native float\n",
        "    return obj\n",
        "\n",
        "explainability_results = check_and_convert_types(explainability_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3DynISbC1LU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Write to file\n",
        "with open('explainability_results_esnli.json', 'w') as f:\n",
        "    json.dump(explainability_results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oyKeUHSsCjm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Write to file\n",
        "with open('explainability_results_cose_large_2.json', 'w') as f:\n",
        "    json.dump(explainability_results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9lmTUmVsTpD"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('explainability_results_cose_large_2.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGy-RsYRCyva"
      },
      "outputs": [],
      "source": [
        "def print_key_tree(d, indent=0):\n",
        "    for key, value in d.items():\n",
        "        print('  ' * indent + str(key))\n",
        "        if isinstance(value, dict):\n",
        "            print_key_tree(value, indent + 1)\n",
        "        elif isinstance(value, list):\n",
        "            if all(isinstance(i, dict) for i in value):\n",
        "                for sub_dict in value:\n",
        "                    print_key_tree(sub_dict, indent + 1)\n",
        "\n",
        "print_key_tree(explainability_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Zba59hyRcOLt",
        "outputId": "d270a6ce-865f-4156-efeb-dd11c152a892"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_a691e4b1-7945-4a6c-a156-3930692ce3f9\", \"explainability_results_esnli.json\", 252136)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('explainability_results_esnli.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6VABQrXDQk"
      },
      "source": [
        "## Logging predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlAC2xecXNlR",
        "outputId": "1fe8d119-6ba0-455b-df72-d9452fac4763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 20min 52s, sys: 3.2 s, total: 20min 55s\n",
            "Wall time: 5min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "predictions_mnli, predictions_esnli, predictions_cose = [],[],[]\n",
        "\n",
        "for model in models_zsc:\n",
        "  result = model.get_results(dataset_cose['question'], candidate_labels_list=dataset_cose['candidate_labels_list'])\n",
        "  pred = model.get_predictions(result, dataset_cose['candidate_labels_list'])\n",
        "  predictions_cose.append(pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCAFsELJuLmG"
      },
      "outputs": [],
      "source": [
        "for model in models_nli:\n",
        "  pred_mnli = model.get_predictions(dataset_mnli['sentence_pairs'])\n",
        "  pred_esnli = model.get_predictions(dataset_esnli['sentence_pairs'])\n",
        "  predictions_mnli.append(pred_mnli)\n",
        "  predictions_esnli.append(pred_esnli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WkHkljApXFYG",
        "outputId": "6bdae941-b09c-41c5-a16f-b3ad0d4fa361"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_a72f1a05-c6ea-4206-a094-cfb82b0e6000\", \"predictions_for_explanations.json\", 29582)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "results = {\n",
        "    'esnli': {\n",
        "        'true': dataset_esnli['test_labels'],\n",
        "          'xsmall': {\n",
        "              'pred': predictions_esnli[0],\n",
        "              'accuracy': accuracy_score(dataset_esnli['test_labels'], predictions_esnli[0]),\n",
        "              'percision': precision_score(dataset_esnli['test_labels'], predictions_esnli[0], average='macro'),\n",
        "              'recall': recall_score(dataset_esnli['test_labels'], predictions_esnli[0], average='macro'),\n",
        "              'f1': f1_score(dataset_esnli['test_labels'], predictions_esnli[0], average='macro'),\n",
        "          },\n",
        "          'small': {\n",
        "              'pred': predictions_esnli[1],\n",
        "              'accuracy': accuracy_score(dataset_esnli['test_labels'], predictions_esnli[1]),\n",
        "              'percision': precision_score(dataset_esnli['test_labels'], predictions_esnli[1], average='macro'),\n",
        "              'recall': recall_score(dataset_esnli['test_labels'], predictions_esnli[1], average='macro'),\n",
        "              'f1': f1_score(dataset_esnli['test_labels'], predictions_esnli[1], average='macro'),\n",
        "          },\n",
        "          'base': {\n",
        "              'pred': predictions_esnli[2],\n",
        "              'accuracy': accuracy_score(dataset_esnli['test_labels'], predictions_esnli[2]),\n",
        "              'percision': precision_score(dataset_esnli['test_labels'], predictions_esnli[2], average='macro'),\n",
        "              'recall': recall_score(dataset_esnli['test_labels'], predictions_esnli[2], average='macro'),\n",
        "              'f1': f1_score(dataset_esnli['test_labels'], predictions_esnli[2], average='macro'),\n",
        "          },\n",
        "          'large': {\n",
        "              'pred': predictions_esnli[3],\n",
        "              'accuracy': accuracy_score(dataset_esnli['test_labels'], predictions_esnli[3]),\n",
        "              'percision': precision_score(dataset_esnli['test_labels'], predictions_esnli[3], average='macro'),\n",
        "              'recall': recall_score(dataset_esnli['test_labels'], predictions_esnli[3], average='macro'),\n",
        "              'f1': f1_score(dataset_esnli['test_labels'], predictions_esnli[3], average='macro'),\n",
        "          },\n",
        "    },\n",
        "        'mnli': {\n",
        "          'true': dataset_mnli['test_labels'],\n",
        "          'xsmall': {\n",
        "              'pred': predictions_mnli[0],\n",
        "              'accuracy': accuracy_score(dataset_mnli['test_labels'], predictions_mnli[0]),\n",
        "              'percision': precision_score(dataset_mnli['test_labels'], predictions_mnli[0], average='macro'),\n",
        "              'recall': recall_score(dataset_mnli['test_labels'], predictions_mnli[0], average='macro'),\n",
        "              'f1': f1_score(dataset_mnli['test_labels'], predictions_mnli[0], average='macro'),\n",
        "          },\n",
        "          'small': {\n",
        "              'pred': predictions_mnli[1],\n",
        "              'accuracy': accuracy_score(dataset_mnli['test_labels'], predictions_mnli[1]),\n",
        "              'percision': precision_score(dataset_mnli['test_labels'], predictions_mnli[1], average='macro'),\n",
        "              'recall': recall_score(dataset_mnli['test_labels'], predictions_mnli[1], average='macro'),\n",
        "              'f1': f1_score(dataset_mnli['test_labels'], predictions_mnli[1], average='macro'),\n",
        "          },\n",
        "          'base': {\n",
        "              'pred': predictions_mnli[2],\n",
        "              'accuracy': accuracy_score(dataset_mnli['test_labels'], predictions_mnli[2]),\n",
        "              'percision': precision_score(dataset_mnli['test_labels'], predictions_mnli[2], average='macro'),\n",
        "              'recall': recall_score(dataset_mnli['test_labels'], predictions_mnli[2], average='macro'),\n",
        "              'f1': f1_score(dataset_mnli['test_labels'], predictions_mnli[2], average='macro'),\n",
        "          },\n",
        "          'large': {\n",
        "              'pred': predictions_mnli[3],\n",
        "              'accuracy': accuracy_score(dataset_mnli['test_labels'], predictions_mnli[3]),\n",
        "              'percision': precision_score(dataset_mnli['test_labels'], predictions_mnli[3], average='macro'),\n",
        "              'recall': recall_score(dataset_mnli['test_labels'], predictions_mnli[3], average='macro'),\n",
        "              'f1': f1_score(dataset_mnli['test_labels'], predictions_mnli[3], average='macro'),\n",
        "          },\n",
        "    },\n",
        "        'cose': {\n",
        "          'true': dataset_cose['true_labels'],\n",
        "          'xsmall': {\n",
        "              'pred': predictions_cose[0],\n",
        "              'accuracy': accuracy_score(dataset_cose['true_labels'], predictions_cose[0]),\n",
        "          },\n",
        "          'small': {\n",
        "              'pred': predictions_cose[1],\n",
        "              'accuracy': accuracy_score(dataset_cose['true_labels'], predictions_cose[1]),\n",
        "          },\n",
        "          'base': {\n",
        "              'pred': predictions_cose[2],\n",
        "              'accuracy': accuracy_score(dataset_cose['true_labels'], predictions_cose[2]),\n",
        "          },\n",
        "          'large': {\n",
        "              'pred': predictions_cose[3],\n",
        "              'accuracy': accuracy_score(dataset_cose['true_labels'], predictions_cose[3]),\n",
        "          },\n",
        "    },\n",
        "}\n",
        "\n",
        "# results are of type np.int64 which is not serializable so we convert it to int()\n",
        "def convert_numpy_int(item):\n",
        "    if isinstance(item, np.int64):\n",
        "        return int(item)\n",
        "    elif isinstance(item, list):\n",
        "        return [convert_numpy_int(sub_item) for sub_item in item]\n",
        "    elif isinstance(item, dict):\n",
        "        return {key: convert_numpy_int(value) for key, value in item.items()}\n",
        "    else:\n",
        "        return item\n",
        "\n",
        "results_converted = convert_numpy_int(results)\n",
        "\n",
        "\n",
        "# Save the dictionary as a JSON file\n",
        "with open('predictions_for_explanations.json', 'w') as json_file:\n",
        "    json.dump(results_converted, json_file, indent=4)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('predictions_for_explanations.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LImk-Dfucvzi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}