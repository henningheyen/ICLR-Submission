{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/henningheyen/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e983922b3b854fbc9cec3de2ebe0865a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "def make_prompt(n_shots, test_review):\n",
    "\n",
    "    prefix = 'This is a movie review dataset with associated positive or negative sentiment: \\n'\n",
    "\n",
    "    sentences = dataset['train'][0:n_shots]['sentence']\n",
    "    labels = dataset['train'][0:n_shots]['label'] # negative (0) or positive (1)\n",
    "    labels_str = ['negative' if label == 0 else 'positive' for label in labels]\n",
    "\n",
    "    shots = \"\"\n",
    "\n",
    "    for sentence, label in zip(sentences, labels_str):\n",
    "        shots = shots + \"'\" + sentence + \"'\" + ' - ' + label + ', \\n '\n",
    "\n",
    "    return prefix + shots + \"'\" + test_review + \"'\" + ' -'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_text(model_name, prompt, n_generated_tokens=2):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Perform a single inference step\n",
    "    output = model.generate(input_ids, max_length=input_ids.size(1) + n_generated_tokens, num_return_sequences=1)\n",
    "\n",
    "    # Extract and decode the generated token\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the generated token\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "    print(\"Last word: \", generated_text.split()[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_token(prompt, model_name='gpt2', top_k=5, verbose=True):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Perform a single inference step\n",
    "    output = model.forward(input_ids)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(output.logits[0,-1,:], dim=0)\n",
    "\n",
    "    # Get the top 5 tokens and their probabilities\n",
    "    top_probs, top_indices = torch.topk(probs, k=top_k)\n",
    "\n",
    "    # Convert the indices to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "\n",
    "    if verbose:\n",
    "        # Print the top 5 tokens and their probabilities\n",
    "        for i in range(top_k):\n",
    "            print(tokens[i], top_probs[i].item())\n",
    "\n",
    "    return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġnegative\n",
      "Ġpositive\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(tokenizer.convert_ids_to_tokens(4633))\n",
    "print(tokenizer.convert_ids_to_tokens(3967))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = dataset['train'][33]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(n_shots=20, test_review = test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a movie review dataset with associated positive or negative sentiment: \\n'hide new secretions from the parental units ' - negative, \\n 'contains no wit , only labored gags ' - negative, \\n 'that loves its characters and communicates something rather beautiful about human nature ' - positive, \\n 'remains utterly satisfied to remain the same throughout ' - negative, \\n 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ' - negative, \\n 'that 's far too tragic to merit such superficial treatment ' - negative, \\n 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ' - positive, \\n 'of saucy ' - positive, \\n 'a depressed fifteen-year-old 's suicidal poetry ' - negative, \\n 'are more deeply thought through than in most ` right-thinking ' films ' - positive, \\n 'goes to absurd lengths ' - negative, \\n 'for those moviegoers who complain that ` they do n't make movies like they used to anymore ' - negative, \\n 'the part where nothing 's happening , ' - negative, \\n 'saw how bad this movie was ' - negative, \\n 'lend some dignity to a dumb story ' - negative, \\n 'the greatest musicians ' - positive, \\n 'cold movie ' - negative, \\n 'with his usual intelligence and subtlety ' - positive, \\n 'redundant concept ' - negative, \\n 'swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . ' - positive, \\n 'by far the worst movie of the year ' -\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġnegative 0.5850855708122253\n",
      "Ġpositive 0.3389498293399811\n",
      "Ġ 0.004583791363984346\n",
      "Ġneutral 0.0024384758435189724\n",
      "Ġ' 0.0015141303883865476\n"
     ]
    }
   ],
   "source": [
    "probs = predict_token(prompt, model_name='gpt2', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = predict_token(prompt, model_name='gpt2-large', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_lime(prompts):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "\n",
    "        probs = predict_token(prompt, verbose=False)\n",
    "\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "        token_probs = probs[tokenizer.convert_tokens_to_ids(['Ġnegative', 'Ġpositive'])] # 3967: id for Ġpositive, 4633: id for Ġnegative\n",
    "        \n",
    "        norm_probs = token_probs / torch.sum(token_probs) # normalizing\n",
    "\n",
    "        results.append([norm_probs[0].item(), norm_probs[1].item()])\n",
    "\n",
    "    return np.array(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63318521, 0.36681476]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_lime([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names = ['negative', 'positive']\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "\n",
    "explanation = explainer.explain_instance(prompt, predict_for_lime, num_samples=10)\n",
    "print(\"Prompt: \", prompt)\n",
    "print(\"LIME Explanation:\")\n",
    "explanation.show_in_notebook(text=True)\n",
    "print('-----------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
