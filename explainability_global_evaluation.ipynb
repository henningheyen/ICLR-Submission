{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Global Expainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_test_set_esnli, make_test_set_mnli, make_test_set_cose\n",
    "from model import ZeroShotNLI, ZeroShotLearner\n",
    "from explainer import Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/Users/henningheyen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a8f4aa1a1d42a5890d31bca04a716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cos_e (/Users/henningheyen/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/e8dc57a5b321a2a97063efb8d316d6d8a0d9a2d3a392dafc913e55bed42736d2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b62ba8dbe74e708d106776fd2e8148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of explanations\n",
    "NUM_EXPL = 10\n",
    "\n",
    "#importing dataset\n",
    "dataset_esnli = make_test_set_esnli(size=NUM_EXPL) # 9830 is dev size esnli (originally 9842 but 12 instance are wrongly annotated)\n",
    "dataset_mnli = make_test_set_mnli(size=NUM_EXPL) # 9815 is dev size mnli\n",
    "dataset_cose = make_test_set_cose(size=NUM_EXPL) # 718 (originally 1221 is dev size cose but 503 instances have bad explanations, i.e. the whole question highlighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cos_e (/Users/henningheyen/.cache/huggingface/datasets/cos_e/v1.11/1.11.0/e8dc57a5b321a2a97063efb8d316d6d8a0d9a2d3a392dafc913e55bed42736d2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c6f28aefc246e38d15e674c2f96ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average explanation input ratio e-SNLI:  0.19858092123245383\n",
      "average explanation input ratio CoS-e:  0.2605110957380536\n"
     ]
    }
   ],
   "source": [
    "#calculating average explanation length per dataset\n",
    "import numpy as np\n",
    "\n",
    "full_dataset_esnli = make_test_set_esnli(size=9830, remove_bad_explanations=True) # 9830 is dev size esnli (originally 9842 but 12 instance are wrongly annotated)\n",
    "full_dataset_cose = make_test_set_cose(size=718, remove_bad_explanations=True) # 718 (originally 1221 is dev size cose but 503 instances have bad explanations, i.e. the whole question highlighted))\n",
    "\n",
    "len_esnli = [len(full_dataset_esnli['extractive_explanation'][i])/(len(full_dataset_esnli['sentence_pairs'][i][0].split())+len(full_dataset_esnli['sentence_pairs'][i][1].split())) for i in range(9830)]\n",
    "len_cose = [len(full_dataset_cose['extractive_explanation'][i])/len(full_dataset_cose['question'][i].split()) for i in range(718)]\n",
    "\n",
    "avg_len_esnli = np.mean(len_esnli)\n",
    "avg_len_cose = np.mean(len_cose)\n",
    "\n",
    "print('average explanation input ratio e-SNLI: ', avg_len_esnli)\n",
    "print('average explanation input ratio CoS-e: ', avg_len_cose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Inference\n",
    "xsmall = ZeroShotNLI(model_name='nli-deberta-v3-xsmall')\n",
    "small = ZeroShotNLI(model_name='nli-deberta-v3-small')\n",
    "base = ZeroShotNLI(model_name='nli-deberta-v3-base')\n",
    "large = ZeroShotNLI(model_name='nli-deberta-v3-large')\n",
    "\n",
    "models = [\n",
    "    xsmall,\n",
    "    small,\n",
    "    base,\n",
    "    large\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    'xsmall',\n",
    "    'small',\n",
    "    'base',\n",
    "    'large'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(class_names=['contradiction', 'entailment', 'neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on xsmall\n",
    "explanations_xsmall = explainer.compute_explanations(\n",
    "    sentences = dataset_mnli['sentence_pairs'], \n",
    "    model=xsmall, \n",
    "    num_samples=100,  \n",
    "    task='NLI',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 54s, sys: 8.85 s, total: 8min 3s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on small\n",
    "explanations_small = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=small, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 21s, sys: 25.7 s, total: 16min 47s\n",
      "Wall time: 16min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on base\n",
    "explanations_base = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=base, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on large\n",
    "explanations_large = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=large, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_mnli = [\n",
    "    explanations_xsmall, \n",
    "    explanations_small, \n",
    "    explanations_base, \n",
    "    explanations_large,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e-SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 3.68 s, total: 3min 53s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on xsmall\n",
    "explanations_xsmall = explainer.compute_explanations(\n",
    "    sentences = dataset_mnli['sentence_pairs'], \n",
    "    model=xsmall, \n",
    "    num_samples=100,  \n",
    "    task='NLI',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 54s, sys: 8.85 s, total: 8min 3s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on small\n",
    "explanations_small = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=small, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 21s, sys: 25.7 s, total: 16min 47s\n",
      "Wall time: 16min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on base\n",
    "explanations_base = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=base, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on large\n",
    "explanations_large = explainer.compute_explanations(\n",
    "   sentences = dataset_mnli['sentence_pairs'], \n",
    "   model=large, \n",
    "   num_samples=100,  \n",
    "   task='NLI',\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_mnli = [\n",
    "    explanations_xsmall, \n",
    "    explanations_small, \n",
    "    explanations_base, \n",
    "    explanations_large,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot Classification\n",
    "xsmall = ZeroShotLearner(model_name='cross-encoder/nli-deberta-v3-xsmall')\n",
    "small = ZeroShotLearner(model_name='cross-encoder/nli-deberta-v3-small')\n",
    "base = ZeroShotLearner(model_name='cross-encoder/nli-deberta-v3-base')\n",
    "large = ZeroShotLearner(model_name='cross-encoder/nli-deberta-v3-large')\n",
    "\n",
    "models = [\n",
    "    xsmall,\n",
    "    small,\n",
    "    base,\n",
    "    large,\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    'xsmall',\n",
    "    'small',\n",
    "    'base',\n",
    "    'large',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no class names becuase of zero shot classification setting\n",
    "explainer = Explainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on xsmall\n",
    "explanations_xsmall = explainer.compute_explanations(\n",
    "   sentences = dataset_cose['question'], \n",
    "   model=model, \n",
    "   num_samples=100,  \n",
    "   class_names_list=dataset_cose['candidate_labels_list']\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on small\n",
    "explanations_small = explainer.compute_explanations(\n",
    "   sentences = dataset_cose['question'], \n",
    "   model=model, \n",
    "   num_samples=100,  \n",
    "   class_names_list=dataset_cose['candidate_labels_list']\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on base\n",
    "explanations_base = explainer.compute_explanations(\n",
    "   sentences = dataset_cose['question'], \n",
    "   model=model, \n",
    "   num_samples=100,  \n",
    "   class_names_list=dataset_cose['candidate_labels_list']\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Computing LIME explanations on large\n",
    "explanations_large = explainer.compute_explanations(\n",
    "   sentences = dataset_cose['question'], \n",
    "   model=model, \n",
    "   num_samples=100,  \n",
    "   class_names_list=dataset_cose['candidate_labels_list']\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_cose = [\n",
    "    explanations_xsmall, \n",
    "    explanations_small, \n",
    "    explanations_base, \n",
    "    explanations_large,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness (Comprehensiveness and Sufficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating aggregated comprehensiveness and sufficiency on 100 explanations\n",
    "comp_list_mnli = []\n",
    "suff_list_mnli = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    print('model: ', model_names[i])\n",
    "\n",
    "    comp_agg = [explainer.aggregated_metric(metric='comprehensiveness', explanation=explanations_mnli[i][j], sentence=dataset_mnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
    "\n",
    "    comp_list_mnli.append(comp_agg)\n",
    "\n",
    "    suff_agg = [explainer.aggregated_metric(metric='sufficiency', explanation=explanations_mnli[i][j], sentence=dataset_mnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
    "\n",
    "    suff_list_mnli.append(suff_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'MNLI average aggregated comprehensiveness {model_names[i]}: ', np.mean(comp_list_mnli[i]))\n",
    "    print(f'MNLI average aggregated sufficiency {model_names[i]}: ', np.mean(suff_list_mnli[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e-SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating aggregated comprehensiveness and sufficiency on 100 explanations\n",
    "comp_list_esnli = []\n",
    "suff_list_esnli = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    print('model: ', model_names[i])\n",
    "\n",
    "    comp_agg = [explainer.aggregated_metric(metric='comprehensiveness', explanation=explanations_mnli[i][j], sentence=dataset_esnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
    "\n",
    "    comp_list_esnli.append(comp_agg)\n",
    "\n",
    "    suff_agg = [explainer.aggregated_metric(metric='sufficiency', explanation=explanations_mnli[i][j], sentence=dataset_esnli['sentence_pairs'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='NLI') for j in range(NUM_EXPL)]\n",
    "\n",
    "    suff_list_esnli.append(suff_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'e-SNLI average aggregated comprehensiveness {model_names[i]}: ', np.mean(comp_list_esnli[i]))\n",
    "    print(f'e-SNLI average aggregated sufficiency {model_names[i]}: ', np.mean(suff_list_esnli[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CoS-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating aggregated comprehensiveness and sufficiency on 100 explanations\n",
    "comp_list_cose = []\n",
    "suff_list_cose = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    print('model: ', model_names[i])\n",
    "\n",
    "    comp_agg = [explainer.aggregated_metric(metric='comprehensiveness', explanation=explanations_mnli[i][j], sentence=dataset_cose['question'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='ZSC', candidate_labels=dataset_cose['candidate_labels_list'][j]) for j in range(NUM_EXPL)]\n",
    "\n",
    "    comp_list_cose.append(comp_agg)\n",
    "\n",
    "    suff_agg = [explainer.aggregated_metric(metric='sufficiency', explanation=explanations_mnli[i][j], sentence=dataset_cose['question'][j], predict=model.predict, verbose=False, bins=[0.1,0.3,0.5], task='ZSC', candidate_labels=dataset_cose['candidate_labels_list'][j]) for j in range(NUM_EXPL)]\n",
    "\n",
    "    suff_list_cose.append(suff_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f'CoS-e average aggregated comprehensiveness {model_names[i]}: ', np.mean(comp_list_cose[i]))\n",
    "    print(f'CoS-e average aggregated sufficiency {model_names[i]}: ', np.mean(suff_list_cose[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plausibility (IOU and Token Level F1 Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e-SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_tokens_lists = [explainer.get_explanation_list(explanations_esnli[i], top_percent=avg_len_esnli) for i in range(4)]\n",
    "ground_truth_list = dataset_esnli['extractive_explanation']\n",
    "\n",
    "iou_macro_scores = [explainer.compute_macro_iou(explanation_tokens_list, ground_truth_list) for explanation_tokens_list in explanation_tokens_lists]\n",
    "tokenf1_macro_scores = [explainer.compute_macro_f1(explanation_tokens_list, ground_truth_list) for explanation_tokens_list in explanation_tokens_lists]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'{model_names[i]} macro_iou for {NUM_EXPL} number of explanations (e-SNLI): ', iou_macro_scores[i])\n",
    "    print(f'{model_names[i]} macro_f1 for {NUM_EXPL} number of explanations (e-SNLI): ', tokenf1_macro_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CoS-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_tokens_lists = [explainer.get_explanation_list(explanations_cose[i], top_percent=avg_len_cose) for i in range(4)]\n",
    "ground_truth_list = dataset_cose['extractive_explanation']\n",
    "\n",
    "iou_macro_scores = [explainer.compute_macro_iou(explanation_tokens_list, ground_truth_list) for explanation_tokens_list in explanation_tokens_lists]\n",
    "tokenf1_macro_scores = [explainer.compute_macro_f1(explanation_tokens_list, ground_truth_list) for explanation_tokens_list in explanation_tokens_lists]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'{model_names[i]} macro_iou for {NUM_EXPL} number of explanations (CoS-e): ', iou_macro_scores[i])\n",
    "    print(f'{model_names[i]} macro_f1 for {NUM_EXPL} number of explanations (CoS-e): ', tokenf1_macro_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
